{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "197290e2",
   "metadata": {},
   "source": [
    "# REQUIREMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deac5064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trema\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "import statistics as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c73f4cf",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5208a0",
   "metadata": {},
   "source": [
    "## Some useful tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e10afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treatment(ds):\n",
    "    dataset = ds[\"train\"]\n",
    "    # We keep only 1000 observations for ramp up the filter computation\n",
    "    dataset = dataset.select(range(1000))\n",
    "    dataset = dataset.filter(lambda example: example['figure_type'] == 'Graph Plot')\n",
    "    # We keep only 500 observations\n",
    "    dataset = dataset.select(range(500))\n",
    "    # Removing + renaming\n",
    "    dataset = dataset.rename_column(\"mlbcap_short\", \"context\")\n",
    "    columns_to_keep = ['id', 'context']\n",
    "    columns_to_remove = [col for col in dataset.column_names if col not in columns_to_keep]\n",
    "    dataset = dataset.remove_columns(columns_to_remove)\n",
    "    # We add necessary columns to use a BERT model\n",
    "    dataset = dataset.map(\n",
    "        lambda example: {\"question\": \"\",\n",
    "                        \"answers\": {'text': [], 'answer_start': []}}\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98fe5169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_question_answer(ds,question, answer_text, answer_start, n_row):\n",
    "    if answer_start != -1:\n",
    "        ds = ds.map(\n",
    "            lambda example, idx: {\n",
    "                **example,\n",
    "                \"question\": question if idx == n_row else example[\"question\"],\n",
    "                \"answers\": {\n",
    "                    \"text\": [answer_text] if idx == n_row else example[\"answers\"][\"text\"],\n",
    "                    \"answer_start\": [answer_start] if idx == n_row else example[\"answers\"][\"answer_start\"]\n",
    "                }\n",
    "            },\n",
    "            with_indices=True\n",
    "        )\n",
    "        return ds\n",
    "    else:\n",
    "        print(\"⚠️ Réponse non trouvée dans le contexte.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4ed53da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database imported from an existing file\n"
     ]
    }
   ],
   "source": [
    "# We load the dataset\n",
    "if os.path.exists(\"datasetNLP.pkl\"):\n",
    "    with open(\"datasetNLP.pkl\", \"rb\") as f:\n",
    "        raw_datasets = pickle.load(f)\n",
    "    print(\"database imported from an existing file\")\n",
    "else:\n",
    "    with open(\"dataset.pkl\", \"rb\") as f:\n",
    "        ds = pickle.load(f)\n",
    "        raw_datasets = treatment(ds)\n",
    "        with open(\"dataset2.pkl\", \"wb\") as f:\n",
    "            pickle.dump(raw_datasets, f)\n",
    "    print('New file created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e691b8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'context', 'question', 'answers'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ee4f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "6a0e8afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n"
     ]
    }
   ],
   "source": [
    "print(n_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "3b892327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Figure 4: CDF of dependency tree sizes in npm and PyPI. npm packages have larger trees; 20% exceed 100 dependencies, while 20% of PyPI packages exceed 10.'"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ligne à modifier\n",
    "n_row +=1\n",
    "raw_datasets[n_row]['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214cc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 3157.45 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 511159518,\n",
       " 'context': 'Figure 4: CDF of dependency tree sizes in npm and PyPI. npm packages have larger trees; 20% exceed 100 dependencies, while 20% of PyPI packages exceed 10.',\n",
       " 'question': 'What does Figure 4 show about the dependency tree sizes in npm and PyPI?',\n",
       " 'answers': {'answer_start': [56],\n",
       "  'text': ['npm packages have larger trees; 20% exceed 100 dependencies, while 20% of PyPI packages exceed 10.']}}"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = raw_datasets[n_row]['context']\n",
    "question = \"What does Figure 4 show about the dependency tree sizes in npm and PyPI?\"\n",
    "answer_text = \"npm packages have larger trees; 20% exceed 100 dependencies, while 20% of PyPI packages exceed 10.\"\n",
    "\n",
    "answer_start = context.index(answer_text)\n",
    "\n",
    "add_question_answer(raw_datasets, question, answer_text, answer_start)\n",
    "\n",
    "raw_datasets[n_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "f67f4516",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasetNLP.pkl\", \"wb\") as f:\n",
    "    pickle.dump(raw_datasets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasetNLP.pkl\", \"rb\") as f:\n",
    "    raw_datasets = pickle.load(f)\n",
    "\n",
    "with open(\"dataset2.pkl\", \"rb\") as f:\n",
    "    dataset2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb = False\n",
    "\n",
    "if dumb:\n",
    "    # Récupérer les ids existants dans raw_datasets\n",
    "    A = set(raw_datasets['id'])\n",
    "    \n",
    "    # Filtrer dataset2 pour ne garder que les lignes dont l'id n'est pas déjà dans raw_datasets\n",
    "    to_add = dataset2.filter(lambda example: example['id'] not in A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15868ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "178c991b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(n_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7dbe396a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Fig. 6. Numerical solution for λ = 9.89 showing ρ̅_x and ρ̅_m as functions of 'a'. ρ̅_x decreases more rapidly than ρ̅_m.\""
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_row +=1\n",
    "to_add[n_row]['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f44dc6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 367/367 [00:00<00:00, 3702.23 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 511159622,\n",
       " 'context': \"Fig. 6. Numerical solution for λ = 9.89 showing ρ̅_x and ρ̅_m as functions of 'a'. ρ̅_x decreases more rapidly than ρ̅_m.\",\n",
       " 'question': \"What does Figure 6 show regarding the functions of 'a'?\",\n",
       " 'answers': {'answer_start': [8],\n",
       "  'text': [\"Numerical solution for λ = 9.89 showing ρ̅_x and ρ̅_m as functions of 'a'. ρ̅_x decreases more rapidly than ρ̅_m\"]}}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = to_add[n_row]['context']\n",
    "question = \"What does Figure 6 show regarding the functions of 'a'?\"\n",
    "answer_text = \"Numerical solution for λ = 9.89 showing ρ̅_x and ρ̅_m as functions of 'a'. ρ̅_x decreases more rapidly than ρ̅_m\"\n",
    "\n",
    "\n",
    "\n",
    "answer_start = context.index(answer_text)\n",
    "\n",
    "to_add = add_question_answer(to_add, question, answer_text, answer_start, n_row)\n",
    "\n",
    "to_add[n_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd34141",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dumb:\n",
    "    B = concatenate_datasets([raw_datasets, to_add[n_row]])\n",
    "    with open(\"datasetNLP.pkl\", \"wb\") as f:\n",
    "    pickle.dump(B, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f785c61d",
   "metadata": {},
   "source": [
    "# QUESTION ANSWERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "410d3c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trema\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d97472",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasetNLP.pkl\", \"rb\") as f:\n",
    "    raw_datasets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b27e155d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 106\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 27\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "042d413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "287772ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "stride = 40\n",
    "\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "492d32f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 106/106 [00:00<00:00, 1726.82 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(106, 110)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"train\"]), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f90238ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "195bbd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_eval_set = raw_datasets[\"test\"]\n",
    "trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "eval_set = small_eval_set.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"test\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c47d1654",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2f67c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "eval_set_for_model.set_format(\"torch\")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n",
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = trained_model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "643bfac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = outputs.start_logits.cpu().numpy()\n",
    "end_logits = outputs.end_logits.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22fad31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "example_to_features = collections.defaultdict(list)\n",
    "for idx, feature in enumerate(eval_set):\n",
    "    example_to_features[feature[\"example_id\"]].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16403aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "predicted_answers = []\n",
    "\n",
    "for example in small_eval_set:\n",
    "    example_id = example[\"id\"]\n",
    "    context = example[\"context\"]\n",
    "    answers = []\n",
    "\n",
    "    for feature_index in example_to_features[example_id]:\n",
    "        start_logit = start_logits[feature_index]\n",
    "        end_logit = end_logits[feature_index]\n",
    "        offsets = eval_set[\"offset_mapping\"][feature_index]\n",
    "\n",
    "        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                # Skip answers that are not fully in the context\n",
    "                if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                    continue\n",
    "                # Skip answers with a length that is either < 0 or > max_answer_length.\n",
    "                if (\n",
    "                    end_index < start_index\n",
    "                    or end_index - start_index + 1 > max_answer_length\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                answers.append(\n",
    "                    {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "    predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "821471f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.53k/4.53k [00:00<?, ?B/s]\n",
      "Downloading extra modules: 100%|██████████| 3.32k/3.32k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a72a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "theoretical_answers = [\n",
    "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71a16ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "prédiction:θ = 0.05 and varying σ²\n",
      "vraie réponse: ['Optimized UPS closely follows DPS performance in 2×2 and 4×4 MIMO systems']\n",
      "---------------\n",
      "prédiction:Outage performance\n",
      "vraie réponse: ['NOMA with HARQ-CC consistently outperforms OMA for user 1, while OMA performs better for user 2 when T = 1']\n",
      "---------------\n",
      "prédiction:outperforms\n",
      "vraie réponse: ['LVAMP outperforms tied and untied LAMP in NMSE across layers for κ(A) = 15, showing 2-5 dB better NMSE than tied LAMP for networks with >4 layers.']\n",
      "---------------\n",
      "prédiction:Interference decreases\n",
      "vraie réponse: ['Interference decreases as R increases']\n",
      "---------------\n",
      "prédiction:high uncertainty\n",
      "vraie réponse: ['Hα periods align with photometry; B periods correlate with equatorial features.']\n",
      "---------------\n",
      "prédiction:MI, NBL, superfluid unpolar, BEC\n",
      "vraie réponse: ['MI, NBL, superfluid unpolar, BEC']\n",
      "---------------\n",
      "prédiction:improves exponentially\n",
      "vraie réponse: ['MGE improves exponentially while MSE slightly decreases. Optimal balance at α ≈ 0.03 achieves 80% attribution accuracy gains with minimal performance loss.']\n",
      "---------------\n",
      "prédiction:NTT\n",
      "vraie réponse: ['The proposed model']\n",
      "---------------\n",
      "prédiction:decreases with increasing ΔE_F for electron doping\n",
      "vraie réponse: ['E_g decreases with increasing ΔE_F for electron doping, increases for hole doping; transition at ΔE_F = 0.60 eV']\n",
      "---------------\n",
      "prédiction:remains close to ground state\n",
      "vraie réponse: ['electron density remains close to ground state']\n",
      "---------------\n",
      "prédiction:velocity-dependent cross-section\n",
      "vraie réponse: ['the velocity-dependent cross-section']\n",
      "---------------\n",
      "prédiction:running waves\n",
      "vraie réponse: ['indicating running waves']\n",
      "---------------\n",
      "prédiction:parametrization in the Appendix of Ref\n",
      "vraie réponse: ['the parametrization in the Appendix of Ref. [8]']\n",
      "---------------\n",
      "prédiction:truthful reporting is optimal\n",
      "vraie réponse: ['Convexity of G ensures truthful reporting is optimal']\n",
      "---------------\n",
      "prédiction:linear dependence with slopes of 1.02 and 1.98, respectively, matching theoretical predictions\n",
      "vraie réponse: ['linear dependence']\n",
      "---------------\n",
      "prédiction:linearly\n",
      "vraie réponse: ['Polarization decreases linearly with increasing orthohydrogen concentration']\n",
      "---------------\n",
      "prédiction:n\n",
      "vraie réponse: ['n']\n",
      "---------------\n",
      "prédiction:increasing trends\n",
      "vraie réponse: ['increasing trends for τₜ and τₜₕₑᵣₘ with density']\n",
      "---------------\n",
      "prédiction:115\n",
      "vraie réponse: ['R=115 at 400 nm']\n",
      "---------------\n",
      "prédiction:the peak flow (Φ_opt) and optimal density (μ_opt)\n",
      "vraie réponse: ['the peak flow (Φ_opt) and optimal density (μ_opt)']\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print('-'*15)\n",
    "    print(f\"prédiction:{predicted_answers[i]['prediction_text']}\")\n",
    "    print(f\"vraie réponse: {theoretical_answers[i]['answers']['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34677206",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Predictions and/or references don't match the expected format.\nExpected format: {'predictions': {'id': Value(dtype='string', id=None), 'prediction_text': Value(dtype='string', id=None)}, 'references': {'id': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}},\nInput predictions: [{'id': 511159490, 'prediction_text': 'θ = 0.05 and varying σ²'}, {'id': 511159503, 'prediction_text': 'Outage performance'}, {'id': 511159470, 'prediction_text': 'outperforms'}, ..., {'id': 511159478, 'prediction_text': 'to calculate the quantum corrected potential'}, {'id': 511159451, 'prediction_text': 'outperforming'}, {'id': 511159380, 'prediction_text': 'time'}],\nInput references: [{'id': 511159490, 'answers': {'answer_start': [8], 'text': ['Optimized UPS closely follows DPS performance in 2×2 and 4×4 MIMO systems']}}, {'id': 511159503, 'answers': {'answer_start': [92], 'text': ['NOMA with HARQ-CC consistently outperforms OMA for user 1, while OMA performs better for user 2 when T = 1']}}, {'id': 511159470, 'answers': {'answer_start': [11], 'text': ['LVAMP outperforms tied and untied LAMP in NMSE across layers for κ(A) = 15, showing 2-5 dB better NMSE than tied LAMP for networks with >4 layers.']}}, ..., {'id': 511159478, 'answers': {'answer_start': [10], 'text': ['Schematic illustration of the potential trajectory from (v, fa) to (hc, 0), used to calculate the quantum corrected potential, neglecting quantum corrections due to λhφ and λφ.']}}, {'id': 511159451, 'answers': {'answer_start': [60], 'text': ['Our method achieves 82% accuracy with 230 images, outperforming uncertainty sampling (250 images) and random sampling (255 images)']}}, {'id': 511159380, 'answers': {'answer_start': [56], 'text': ['The wavefront error is plotted as a function of time']}}]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredicted_answers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtheoretical_answers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\evaluate\\module.py:455\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    452\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\evaluate\\module.py:546\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    540\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and/or references don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    542\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(references)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m     )\n\u001b[1;32m--> 546\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Predictions and/or references don't match the expected format.\nExpected format: {'predictions': {'id': Value(dtype='string', id=None), 'prediction_text': Value(dtype='string', id=None)}, 'references': {'id': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}},\nInput predictions: [{'id': 511159490, 'prediction_text': 'θ = 0.05 and varying σ²'}, {'id': 511159503, 'prediction_text': 'Outage performance'}, {'id': 511159470, 'prediction_text': 'outperforms'}, ..., {'id': 511159478, 'prediction_text': 'to calculate the quantum corrected potential'}, {'id': 511159451, 'prediction_text': 'outperforming'}, {'id': 511159380, 'prediction_text': 'time'}],\nInput references: [{'id': 511159490, 'answers': {'answer_start': [8], 'text': ['Optimized UPS closely follows DPS performance in 2×2 and 4×4 MIMO systems']}}, {'id': 511159503, 'answers': {'answer_start': [92], 'text': ['NOMA with HARQ-CC consistently outperforms OMA for user 1, while OMA performs better for user 2 when T = 1']}}, {'id': 511159470, 'answers': {'answer_start': [11], 'text': ['LVAMP outperforms tied and untied LAMP in NMSE across layers for κ(A) = 15, showing 2-5 dB better NMSE than tied LAMP for networks with >4 layers.']}}, ..., {'id': 511159478, 'answers': {'answer_start': [10], 'text': ['Schematic illustration of the potential trajectory from (v, fa) to (hc, 0), used to calculate the quantum corrected potential, neglecting quantum corrections due to λhφ and λφ.']}}, {'id': 511159451, 'answers': {'answer_start': [60], 'text': ['Our method achieves 82% accuracy with 230 images, outperforming uncertainty sampling (250 images) and random sampling (255 images)']}}, {'id': 511159380, 'answers': {'answer_start': [56], 'text': ['The wavefront error is plotted as a function of time']}}]"
     ]
    }
   ],
   "source": [
    "metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
